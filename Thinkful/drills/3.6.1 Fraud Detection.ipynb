{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "random.seed(11)\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this notebook, I'll be be using a database of credit card transactions to build an accurate predictor of whether a transaction is fraudulent.  The dataset, downloaded from Kaggle, is described in detail below.\n",
    "\n",
    "Thinkful's instructions: \"Using this credit card fraud dataset develop an algorithm to predict fraud. Prioritize correctly finding fraud rather than correctly labeling non-fraudulent transactions.\"  In light of this, my best metric will be the F1-score, since it doesn't take the number of true negatives into account (whereas an evenly balanced metric like the Matthews Coefficient does).\n",
    "\n",
    "\n",
    "# About the data\n",
    "\n",
    "From Kaggle:\n",
    "\n",
    "\"The dataset contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "\"It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\"\n",
    "\n",
    "It's particularly noteworthy that the dataset does not include an identity feature (so that pattern analysis could be personalized to each user).  The source does not specify whether this was included in the raw data (before PCA) or whether a version of PCA was used that could include a categorical variable of that nature.\n",
    "\n",
    "# Methods\n",
    "\n",
    "To begin with, I'll try a few general classifiers out of the box: logistic regression, random forest, and gradient boost.  Then I'll try adding the results of a couple anomaly detection algorithms as features.  Finally, I'll take the most promising model and optimize it over the complete dataset (including the anomaly flags.)\n",
    "\n",
    "Because the data is already a aset of Principal Components (except for Amount and Time), there's no real opportunity for feature engineering, so my experimentations here will all be algorithmic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and examine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "credit_df = pd.read_csv(\"practice/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 31)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(credit_df.shape)\n",
    "credit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean transaction amount (Euros):  88.35\n",
      "Median transaction amount (Euros):  22.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12050d470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Mean transaction amount (Euros): \", round(credit_df.Amount.mean(), 2))\n",
    "print(\"Median transaction amount (Euros): \", credit_df.Amount.median())\n",
    "x = [amt for amt in credit_df.Amount if amt < 400]\n",
    "sns.distplot(x, bins=20, kde=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = credit_df.drop('Class', axis=1)\n",
    "y = credit_df['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial model trials\n",
    "\n",
    "## Model 1: logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.656\n",
      "Test score:  0.709\n",
      "False negatives:  40\n",
      "False positives:  28\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "lreg = LogisticRegression(penalty='l2')\n",
    "lreg.fit(X_train, y_train)\n",
    "train_pred = lreg.predict(X_train)\n",
    "test_pred = lreg.predict(X_test)\n",
    "\n",
    "false_neg = sum((y_test - test_pred) == 1)\n",
    "false_pos = sum((y_test - test_pred) == -1)\n",
    "print(\"Train score: \", round(f1_score(y_train, train_pred), 3))\n",
    "print(\"Test score: \", round(f1_score(y_test, test_pred),3))\n",
    "print(\"False negatives: \", false_neg)\n",
    "print('False positives: ', false_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.992\n",
      "Test score:  0.856\n",
      "False negatives:  25\n",
      "False positives:  8\n",
      "Elapsed time (s):  39.06005573272705\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from time import time\n",
    "start = time()\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=20)\n",
    "rfc.fit(X_train, y_train)\n",
    "train_pred = rfc.predict(X_train)\n",
    "test_pred = rfc.predict(X_test)\n",
    "\n",
    "false_neg = sum((y_test - test_pred) == 1)\n",
    "false_pos = sum((y_test - test_pred) == -1)\n",
    "print(\"Train score: \", round(f1_score(y_train, train_pred), 3))\n",
    "print(\"Test score: \", round(f1_score(y_test, test_pred),3))\n",
    "print(\"False negatives: \", false_neg)\n",
    "print('False positives: ', false_pos)\n",
    "print('Elapsed time (s): ', time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Gradient Boosted Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.649\n",
      "Test score:  0.648\n",
      "False negatives:  55\n",
      "False positives:  19\n",
      "Elapsed time (s):  155.08902025222778\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "start = time()\n",
    "\n",
    "gbc = GradientBoostingClassifier()\n",
    "gbc.fit(X_train, y_train)\n",
    "train_pred = gbc.predict(X_train)\n",
    "test_pred = gbc.predict(X_test)\n",
    "\n",
    "false_pos = sum((y_test - test_pred) == -1)\n",
    "false_neg = sum((y_test - test_pred) == 1)\n",
    "print(\"Train score: \", round(f1_score(y_train, train_pred), 3))\n",
    "print(\"Test score: \", round(f1_score(y_test, test_pred),3))\n",
    "print(\"False negatives: \", false_neg)\n",
    "print('False positives: ', false_pos)\n",
    "print('Elapsed time (s): ', time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.255\n",
      "Test score:  0.311\n",
      "False negatives:  67\n",
      "False positives:  181\n",
      "Elapsed time (s):  23.06566023826599\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "start = time()\n",
    "fraud_ratio = y_train.sum()/len(y_train)\n",
    "iso = IsolationForest(contamination=fraud_ratio*2) # multiplied by 2 to make this more inclusive (catching false negs)\n",
    "iso.fit(X_train, y_train)\n",
    "train_pred = iso.predict(X_train)\n",
    "test_pred = iso.predict(X_test)\n",
    "\n",
    "# convert +1/-1 output to 0/1:\n",
    "train_pred = (1-train_pred)/2\n",
    "test_pred = (1-test_pred)/2\n",
    "\n",
    "false_pos = sum((y_test - test_pred) == -1)\n",
    "false_neg = sum((y_test - test_pred) == 1)\n",
    "print(\"Train score: \", round(f1_score(y_train, train_pred), 3))\n",
    "print(\"Test score: \", round(f1_score(y_test, test_pred),3))\n",
    "print(\"False negatives: \", false_neg)\n",
    "print('False positives: ', false_pos)\n",
    "print('Elapsed time (s): ', time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I tried running a hyperparameter sweep on this in hopes that the parameters were the problem.  Oddly, it won't run.  (Error reports included after the Conclusion near bottom of notebook.)  Manual tweaking turned up some small increases, but even dramatic decreases didn't seem to make any difference for the amount of help provided to Random Forest.\n",
    "\n",
    "The goal of Isolation Forest in this context is not to itself attain high accuracy, but to add a useful feature (the anomaly flag) to our dataset.  Now let's try Random Forest again with this anomaly flag included and see how it affects our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make version of X_train and X_test with anomaly flag included\n",
    "X_train_a = X_train.copy()\n",
    "X_train_a['isonomaly'] = pd.Series(train_pred).values\n",
    "X_test_a = X_test.copy()\n",
    "X_test_a['isonomaly'] = pd.Series(test_pred).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.986\n",
      "Test score:  0.874\n",
      "False negatives:  22\n",
      "False positives:  7\n",
      "Elapsed time (s):  37.21060299873352\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "rfc = RandomForestClassifier(n_estimators=20)\n",
    "rfc.fit(X_train_a, y_train)\n",
    "train_pred = rfc.predict(X_train_a)\n",
    "test_pred = rfc.predict(X_test_a)\n",
    "\n",
    "false_neg = sum((y_test - test_pred) == 1)\n",
    "false_pos = sum((y_test - test_pred) == -1)\n",
    "print(\"Train score: \", round(f1_score(y_train, train_pred), 3))\n",
    "print(\"Test score: \", round(f1_score(y_test, test_pred),3))\n",
    "print(\"False negatives: \", false_neg)\n",
    "print('False positives: ', false_pos)\n",
    "print('Elapsed time (s): ', time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our score shows only a small improvement with the addition of the Isolation Forest data.  (Our original accuracy for the out-of-box random forest was 0.856.)  Let's try one more anomaly detection algorithm: the Local Outlier Factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5: Local Outlier Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.051\n",
      "Elapsed time (s):  205.15208101272583\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "start = time()\n",
    "lof = LocalOutlierFactor(contamination=fraud_ratio)\n",
    "pred = lof.fit_predict(X)\n",
    "\n",
    "# convert +1/-1 output to 1/0:\n",
    "pred = (1-pred)/2\n",
    "\n",
    "print(\"Score: \", round(f1_score(y, pred), 3))\n",
    "print('Elapsed time (s): ', time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xcore:  0.051\n",
      "False negatives:  467\n",
      "False positives:  467\n"
     ]
    }
   ],
   "source": [
    "false_neg = sum((y - pred) == 1)\n",
    "false_pos = sum((y - pred) == -1)\n",
    "print(\"Score: \", round(f1_score(y, pred), 3))\n",
    "print(\"False negatives: \", false_neg)\n",
    "print('False positives: ', false_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = X.copy()\n",
    "X2['LOFlier'] = pd.Series(pred).values\n",
    "X_traino, X_testo, y_traino, y_testo = train_test_split(X2, y, test_size = 0.25) # the 'o' is for outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.984\n",
      "Test score:  0.837\n",
      "False negatives:  123\n",
      "False positives:  105\n",
      "Elapsed time (s):  36.075578689575195\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "rfc = RandomForestClassifier(n_estimators=20)\n",
    "rfc.fit(X_traino, y_traino)\n",
    "train_pred = rfc.predict(X_traino)\n",
    "test_pred = rfc.predict(X_testo)\n",
    "\n",
    "false_neg = sum((y_test - test_pred) == 1)\n",
    "false_pos = sum((y_test - test_pred) == -1)\n",
    "print(\"Train score: \", round(f1_score(y_traino, train_pred), 3))\n",
    "print(\"Test score: \", round(f1_score(y_testo, test_pred),3))\n",
    "print(\"False negatives: \", false_neg)\n",
    "print('False positives: ', false_pos)\n",
    "print('Elapsed time (s): ', time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This actually scored lower than our out-of-box version, but it's a new split - unfortunately, it's impossible to run the LOF algorithm post-split.  And this score is within the range that results from different splits; the out-of-box RF score has varied from 0.825 to 0.856 with different splits.  So all that's clear is that the LOF flag addition helps very little if it helps at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model choice conclusions\n",
    "\n",
    "Random Forest performed well here.  Boosted Tree didn't perform well on this split, but it performed just as well as Random Forest on one previous split (0.20 better, which is strange). While the anomaly detection / outlier detection algorithms didn't directly capture the frauds very accurately (f1-scores of 0.26 and 0.05), they did contribute (ableit very slightly) to the accuracy of our other models when used as features.\n",
    "\n",
    "Moving forward, I'll try optimizing both Random Forest and Boosted Tree, and I'll include both our anomaly flags in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizations\n",
    "\n",
    "Because Local Outlier Factor needs to be run on the entire dataset pre-split, we'll take the LOF splits above and use those as our regular training data.  This means we'll also need to rerun Isolation Forest to re-generate the isonomaly feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.031\n",
      "Test score:  0.029\n",
      "False negatives:  109\n",
      "False positives:  7192\n",
      "Elapsed time (s):  23.41415309906006\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "fraud_ratio = y_traino.sum()/len(y_traino)\n",
    "iso = IsolationForest(n_estimators=100)\n",
    "iso.fit(X_traino.drop('LOFlier', axis=1), y_traino)\n",
    "train_pred = iso.predict(X_traino.drop('LOFlier', axis=1))\n",
    "test_pred = iso.predict(X_testo.drop('LOFlier', axis=1))\n",
    "\n",
    "# convert +1/-1 output to 0/1:\n",
    "train_pred = (1-train_pred)/2\n",
    "test_pred = (1-test_pred)/2\n",
    "\n",
    "false_pos = sum((y_test - test_pred) == -1)\n",
    "false_neg = sum((y_test - test_pred) == 1)\n",
    "print(\"Train score: \", round(f1_score(y_traino, train_pred), 3))\n",
    "print(\"Test score: \", round(f1_score(y_testo, test_pred),3))\n",
    "print(\"False negatives: \", false_neg)\n",
    "print('False positives: ', false_pos)\n",
    "print('Elapsed time (s): ', time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "First, let's run a hyperparameter sweep and see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report utility function from http://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed seconds:  4084.3675079345703\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.873 (std: 0.026)\n",
      "Parameters: {'random_state': 11, 'n_estimators': 30, 'min_samples_split': 20, 'max_features': 20, 'max_depth': 8, 'criterion': 'entropy', 'bootstrap': False}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.871 (std: 0.020)\n",
      "Parameters: {'random_state': 11, 'n_estimators': 20, 'min_samples_split': 10, 'max_features': 6, 'max_depth': 20, 'criterion': 'entropy', 'bootstrap': False}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.865 (std: 0.025)\n",
      "Parameters: {'random_state': 11, 'n_estimators': 30, 'min_samples_split': 2, 'max_features': 9, 'max_depth': 20, 'criterion': 'gini', 'bootstrap': False}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "rfc_2 = RandomForestClassifier()\n",
    "\n",
    "param_dist = {\"n_estimators\":[20, 30],\n",
    "              \"random_state\":[11],\n",
    "              \"max_depth\": [4,8,20],\n",
    "              \"max_features\": [6,9,12,20],\n",
    "              \"min_samples_split\": [2,10,20],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(rfc_2, param_distributions=param_dist, n_iter=20, cv=4, scoring='f1')\n",
    "start = time()\n",
    "random_search.fit(X_traino, y_traino)\n",
    "\n",
    "print(\"Elapsed seconds: \", time()-start)\n",
    "report(random_search.cv_results_, n_top=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have several equally good scores that are all somewhat overfit.  Let's take these general parameters and look at our test score, false negatives, and false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.925\n",
      "Test score:  0.829\n",
      "False negatives:  25\n",
      "False positives:  15\n",
      "Elapsed time (s):  206.94753098487854\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "params = {'random_state': 11, 'n_estimators': 30, 'min_samples_split': 20, 'max_features': 20, 'max_depth': 8, 'criterion': 'entropy', 'bootstrap': False}\n",
    "\n",
    "rfc_opt = RandomForestClassifier(**params)\n",
    "rfc_opt.fit(X_traino, y_traino)\n",
    "train_pred = rfc_opt.predict(X_traino)\n",
    "test_pred = rfc_opt.predict(X_testo)\n",
    "\n",
    "false_pos = sum((y_testo - test_pred) == -1)\n",
    "false_neg = sum((y_testo - test_pred) == 1)\n",
    "print(\"Train score: \", round(f1_score(y_traino, train_pred), 3))\n",
    "print(\"Test score: \", round(f1_score(y_testo, test_pred),3))\n",
    "print(\"False negatives: \", false_neg)\n",
    "print('False positives: ', false_pos)\n",
    "print('Elapsed time (s): ', time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The good news here is that our f1-score is pretty solid at 0.88.  The bad news is that we still have 22 false negatives (out of 129 actual negatives), which means we're misidentifying 17% of fraudulent transactions.  Given the cost of failing to identify fraudulent transactions, identifying 83% of frauds probably isn't good enough.\n",
    "\n",
    "We may be able to improve on this by changing our algorithm.  By adjusting the class weights of the random tree, we can heavily prioritize correctly identifying positive cases (the minority case) over negative cases.  Let's try over-weighting fraud identification by increasing powers of 10 and see what happens to our false negatives and our f1-scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time (s):  404.0913248062134\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "scores = []\n",
    "false_neg_counts = []\n",
    "false_pos_counts = []\n",
    "balance_exps = [0, 2, 4, 6, 8, 10, 12]\n",
    "for balance_exp in balance_exps:\n",
    "    params = {'random_state': 11, \n",
    "              'n_estimators': 20, \n",
    "              'min_samples_split': 20, \n",
    "              'max_features': 20, \n",
    "              'max_depth': 8, \n",
    "              'criterion': 'entropy', \n",
    "              'bootstrap': False,\n",
    "              'class_weight': {0:1, 1:10**balance_exp}\n",
    "             }\n",
    "\n",
    "    rfc = RandomForestClassifier(**params)\n",
    "    rfc.fit(X_traino, y_traino)\n",
    "    train_pred = rfc.predict(X_traino)\n",
    "    test_pred = rfc.predict(X_testo)\n",
    "\n",
    "    false_neg_counts.append( sum((y_testo - test_pred) == 1) )\n",
    "    false_pos_counts.append( sum((y_testo - test_pred) == -1) )\n",
    "    scores.append( round(f1_score(y_traino, train_pred), 3) )\n",
    "\n",
    "print('Elapsed time (s): ', time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e938860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(balance_exps, false_neg_counts, label='false negatives')\n",
    "plt.plot(balance_exps, false_pos_counts, label='false positives')\n",
    "plt.plot(balance_exps, [s*10 for s in scores], label='f1-scores * 10')\n",
    "plt.title(\"class_weight exponent vs model metrics\")\n",
    "plt.xlabel('log of weight imbalance')\n",
    "plt.ylabel('performance metrics')\n",
    "plt.legend()\n",
    "plt.ylim(0,40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the tradeoff in accuracy is way too steep here: our f1-score drops below 0.5 before we see any significant reduction in false negatives.\n",
    "\n",
    "For now, then, our best try is 83% fraud detection with an f1-score of 89%.  Let's table that for now and try optimizing our gradient boosted tree model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Gradient Boosted Tree\n",
    "\n",
    "We'll start by optimizing all parameters except the computation-intensive n_estimators and learning_rate, then hold the others constant while we tweak those two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed seconds:  800.3299012184143\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.804 (std: 0.036)\n",
      "Parameters: {'subsample': 0.6, 'random_state': 11, 'n_estimators': 50, 'max_features': 6, 'max_depth': 6, 'loss': 'exponential', 'learning_rate': 0.1}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.801 (std: 0.028)\n",
      "Parameters: {'subsample': 0.6, 'random_state': 11, 'n_estimators': 50, 'max_features': 6, 'max_depth': 4, 'loss': 'exponential', 'learning_rate': 0.1}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.796 (std: 0.030)\n",
      "Parameters: {'subsample': 1, 'random_state': 11, 'n_estimators': 50, 'max_features': 4, 'max_depth': 6, 'loss': 'exponential', 'learning_rate': 0.1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from time import time\n",
    "\n",
    "gbc_2 = GradientBoostingClassifier()\n",
    "\n",
    "param_dist = {\n",
    "    'loss': ['deviance','exponential'],\n",
    "    'n_estimators': [50], # we'll change this next time\n",
    "    'learning_rate': [0.1], # ditto\n",
    "    'max_depth': [2,4,6,8],\n",
    "    'max_features': [2,4,6,8],\n",
    "    'subsample': [0.1, 0.3, 0.6, 1],\n",
    "    'random_state': [11]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(gbc_2, param_distributions=param_dist, n_iter=25, scoring='f1', cv=4)\n",
    "start = time()\n",
    "random_search.fit(X_traino, y_traino)\n",
    "\n",
    "print(\"Elapsed seconds: \", time()-start)\n",
    "report(random_search.cv_results_, n_top=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I ran the above search with the default cv=3, which is why the scores are a little lower.  It doesn't change the ranking, which is what we're concerned about here, so I'm using the results as they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed seconds:  1937.0215361118317\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.872 (std: 0.023)\n",
      "Parameters: {'subsample': 0.6, 'random_state': 11, 'n_estimators': 130, 'max_features': 6, 'max_depth': 6, 'loss': 'exponential', 'learning_rate': 0.18000000000000005}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.872 (std: 0.027)\n",
      "Parameters: {'subsample': 0.6, 'random_state': 11, 'n_estimators': 110, 'max_features': 6, 'max_depth': 6, 'loss': 'exponential', 'learning_rate': 0.18000000000000005}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.867 (std: 0.023)\n",
      "Parameters: {'subsample': 0.6, 'random_state': 11, 'n_estimators': 130, 'max_features': 6, 'max_depth': 6, 'loss': 'exponential', 'learning_rate': 0.16000000000000003}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from time import time\n",
    "param_dist = {\n",
    "    'loss': ['exponential'],\n",
    "    'n_estimators': range(50, 200, 10),\n",
    "    'learning_rate': np.arange(0.05, 0.2, 0.01),\n",
    "    'max_depth': [6],\n",
    "    'max_features': [6],\n",
    "    'subsample': [0.6],\n",
    "    'random_state': [11]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(gbc_2, param_distributions=param_dist, n_iter=10, scoring='f1', cv=4)\n",
    "start = time()\n",
    "random_search.fit(X_traino, y_traino)\n",
    "\n",
    "print(\"Elapsed seconds: \", time()-start)\n",
    "report(random_search.cv_results_, n_top=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.971\n",
      "Test score:  0.85\n",
      "False negatives:  26\n",
      "False positives:  8\n",
      "Elapsed time (s):  99.17459917068481\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "params = {'random_state': 11, 'subsample': 0.6, 'n_estimators': 130, 'max_features': 6, 'max_depth': 6, 'loss': 'exponential', 'learning_rate': 0.18}\n",
    "\n",
    "gbc_3 = GradientBoostingClassifier(**params)\n",
    "gbc_3.fit(X_traino, y_traino)\n",
    "train_pred = gbc_3.predict(X_traino)\n",
    "test_pred = gbc_3.predict(X_testo)\n",
    "\n",
    "false_pos = sum((y_testo - test_pred) == -1)\n",
    "false_neg = sum((y_testo - test_pred) == 1)\n",
    "print(\"Train score: \", round(f1_score(y_traino, train_pred), 3))\n",
    "print(\"Test score: \", round(f1_score(y_testo, test_pred),3))\n",
    "print(\"False negatives: \", false_neg)\n",
    "print('False positives: ', false_pos)\n",
    "print('Elapsed time (s): ', time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that gradient boosted tree will yield basically the same results as random forest.  For now, we'll use gradient boosted tree as our top performer moving forward.  But they're basically identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final boosted tree test\n",
    "\n",
    "Our best model on our test set has been our gradient boosted tree.  Let's make sure this replicates across different random splits of our data (i.e., make sure it's not overfit to our test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False negatives mean:  26.75\n",
      "False negatives variance:  18.188\n",
      "False positives mean:  4.25\n",
      "Mean f1-score: 0.861\n",
      "Elapsed time (s):  92.85223007202148\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "false_neg_counts = []\n",
    "false_pos_counts = []\n",
    "scores = []\n",
    "\n",
    "for i in range(4):\n",
    "    random.seed(a=i)\n",
    "    X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "    start = time()\n",
    "    params = {'random_state': i, 'n_estimators': 100, 'max_features': 6, 'max_depth': 6, 'loss': 'exponential', 'learning_rate': 0.2}\n",
    "\n",
    "    gbc_4 = GradientBoostingClassifier(**params)\n",
    "    gbc_4.fit(X_train2, y_train2)\n",
    "    train_pred = gbc_4.predict(X_train2)\n",
    "    test_pred = gbc_4.predict(X_test2)\n",
    "    \n",
    "    false_pos = sum((y_test2 - test_pred) == -1)\n",
    "    false_negs = sum((y_test2 - test_pred) == 1)\n",
    "    score = round(f1_score(y_test2, test_pred),3)\n",
    "    \n",
    "    false_pos_counts.append(false_pos)\n",
    "    false_neg_counts.append(false_negs)\n",
    "    scores.append(score)\n",
    "    \n",
    "print(\"False negatives mean: \", round(np.mean(false_neg_counts),3) )\n",
    "print(\"False negatives variance: \", round(np.var(false_neg_counts),3) )\n",
    "print(\"False positives mean: \", round(np.mean(false_pos_counts),3) )\n",
    "print(\"Mean f1-score:\", round(np.mean(scores), 3) )\n",
    "print('Elapsed time (s): ', time()-start)\n",
    "random.seed(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23, 31, 22, 31]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_neg_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In light of these results, my recommendation would probably be for the bank in this case to adopt a hybrid model.\n",
    "\n",
    "The primary model would be the gradient boosted tree above, which catches about 83% of fraudulent transactions with an a miniscule rate of false positives (f1-score = 0.86).  Transactions flagged by this fraud detector should be denied.\n",
    "\n",
    "The secondary should be a model that flags nearly all of the fraudulent transactions at the cost of also incorrectly flagging many non-fraudulent transactions.  We might, for instance, use our heavily weighted random forest model, which flagged 94% of fraudulent transactions, at the cost of having 98% of its flags be false flags.  For transactions flagged only by the seondary model, the transaction should only be denied if it is above a certain amount (e.g., 2,000 USD) above which the risk of a fraudulent transaction is unacceptable, or if it is the third or more consecutive secondary-flagged transaction (which would only occur in approximately 0.1 percent of all transactions).\n",
    "\n",
    "Together, these models will catch 83% of fraudulent transactions, and halt an additional 11% of fraudulent transactions if they are large transactions.  This means that only 6% of large fraudulent transactions, and between 6% and 17% of all fraudulent transactions, would go unflagged.  All told, this represent a very high reduction of risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "<ol>\n",
    "    <li>It strikes me that this is a case where I probably should have divided my dataset three ways into a training set, cv set, and test set, rather than a simple train_test split.  I compensated for this by testing against several new random splits at the end, which is almost (but not totally) as good.  Should I have done a three-way split here?  Should I usually do a three-way split?</li>\n",
    "    <li>Are there better ways than weight adjustments (which tanked the f1-score) to prioritize eliminating false positives?  (I get the impression that thresholding is not ideal.  And according to this paper, weighting and subsampling perform almost identically: http://statistics.berkeley.edu/sites/default/files/tech-reports/666.pdf)</li>\n",
    "    <li>Any ideas on debugging the hyperperparameter sweep of Isolation Forest (below) so that it runs?</li>\n",
    "</ol>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolation Forest hyperparameter sweep failure\n",
    "\n",
    "Because this is a really interesting and much-touted model, I decided to run a hyperparameter sweep to see if other parameter settings could yield at least a not-egregiously-bad score.  This resulted in a pretty baffling bug:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target is multiclass but average='binary'. Please choose another average setting.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-0d9cf5ab7407>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mrandom_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miso_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_distributions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Elapsed seconds: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0mfit_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;31m# _score will return dict if is_multimetric is True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0mtest_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_multimetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfit_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer, is_multimetric)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \"\"\"\n\u001b[1;32m    522\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_multimetric\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_multimetric_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_multimetric_score\u001b[0;34m(estimator, X_test, y_test, scorers)\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'item'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/metrics/scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             return self._sign * self._score_func(y_true, y_pred,\n\u001b[0;32m--> 108\u001b[0;31m                                                  **self._kwargs)\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m    712\u001b[0m     return fbeta_score(y_true, y_pred, 1, labels=labels,\n\u001b[1;32m    713\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m                        sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m    826\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f-score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    829\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m             raise ValueError(\"Target is %s but average='binary'. Please \"\n\u001b[0;32m-> 1040\u001b[0;31m                              \"choose another average setting.\" % y_type)\n\u001b[0m\u001b[1;32m   1041\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mpos_label\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m         warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n",
      "\u001b[0;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "iso_2 = IsolationForest()\n",
    "\n",
    "param_dist = {\"n_estimators\":range(20,2000), \n",
    "              \"max_samples\":range(100,2000), \n",
    "              \"max_features\":range(1,20), \n",
    "              \"contamination\":[fraud_ratio], \n",
    "              \"bootstrap\":[False, True]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(iso_2, param_distributions=param_dist, n_iter=30, scoring='f1')\n",
    "start = time()\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Elapsed seconds: \", time()-start)\n",
    "report(random_search.cv_results_, n_top=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This error is pretty baffling given that both the predictions and y_train are verifiably binary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tp = pd.Series(test_pred)\n",
    "tp.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we try using a multiclass metric with the average specified..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # trying this did nothing\n",
    "# from sklearn.preprocessing import label_binarize\n",
    "# y2 = label_binarize(y_train, classes=[0, 1])\n",
    "\n",
    "iso_2 = IsolationForest()\n",
    "\n",
    "param_dist = {\"n_estimators\":[50], \n",
    "              \"max_samples\":[300], \n",
    "              \"max_features\":[1], \n",
    "              \"contamination\":[fraud_ratio], \n",
    "              \"bootstrap\":[False, True]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(iso_2, param_distributions=param_dist, n_iter=2, scoring='f1_macro')\n",
    "start = time()\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Elapsed seconds: \", time()-start)\n",
    "report(random_search.cv_results_, n_top=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't like that either.  So I'm pretty much at a dead end here: the out-of-box performance is poor, and I can't get a hyperparameter sweep to run due to what sure looks like some sort of bug. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Everything below here is scratch -- ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack 1: final boost like boosted tree\n",
    "rfc = RandomForestClassifier(n_estimators=40)\n",
    "rfc.fit(X_train, y_train)\n",
    "train_pred = rfc.predict(X_train)\n",
    "test_pred = rfc.predict(X_test)\n",
    "errors = train_pred-y_train\n",
    "\n",
    "rfc2 = RandomForestClassifier(n_estimators=40)\n",
    "rfc2.fit(X_train, errors)\n",
    "error_train_pred = rfc2.predict(X_train)\n",
    "error_test_pred = rfc2.predict(X_test)\n",
    "\n",
    "final_train_pred = train_pred - error_train_pred\n",
    "final_test_pred = test_pred - error_test_pred\n",
    "print(\"Train score: \", round(f1_score(y_train, final_train_pred), 3))\n",
    "print(\"Test score: \", round(f1_score(y_test, final_test_pred),3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRY STACKING HERE (one boost after the RF - or more?)\n",
    "\n",
    "also JUST REALIZED that NNs are just stacked linear regressions.  What happens if you stack RFs in multiple layers???\n",
    "\n",
    "OR BETTER YET: figure out how to combine RFs into a NN-analogous structure\n",
    "A backprop equivalent is, I think, the key here.  Could you use something like gradient boost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack 2: use outputs from trees as input\n",
    "# part 1: generate tree predictions\n",
    "train_predictions = []\n",
    "test_predictions = []\n",
    "forests = 20\n",
    "for i in range(forests):\n",
    "    rfc = RandomForestClassifier(n_estimators=5)\n",
    "    rfc.fit(X_train, y_train)\n",
    "    train_predictions.append(rfc.predict(X_train))\n",
    "    test_predictions.append(rfc.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimental part 3: double-stack, like a NN\n",
    "train_predictions2 = []\n",
    "test_predictions2 = []\n",
    "forests = 20\n",
    "for i in range(forests):\n",
    "    rfc = RandomForestClassifier(n_estimators=5)\n",
    "    rfc.fit(train_predictions, y_train)\n",
    "    train_predictions2.append(rfc.predict(train_predictions))\n",
    "    test_predictions2.append(rfc.predict(test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 2: train model on those predictions\n",
    "rfc = RandomForestClassifier(n_estimators=80)\n",
    "rfc.fit(predictions, y_train)\n",
    "train_pred = rfc.predict(train_predictions)\n",
    "test_pred = rfc.predict(test_predictions)\n",
    "\n",
    "false_pos = sum((y_test - test_pred) == -1)\n",
    "false_neg = sum((y_test - test_pred) == 1)\n",
    "print(\"Train score: \", round(f1_score(y_train, train_pred), 3))\n",
    "print(\"Test score: \", round(f1_score(y_test, test_pred),3))\n",
    "print(\"False negatives: \", false_neg)\n",
    "print('False positives: ', false_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fun, take just the top two PCs and plot the gradient and shape as in the sklearn examples.  This might also help diagnose the problems with my anomaly detection algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
