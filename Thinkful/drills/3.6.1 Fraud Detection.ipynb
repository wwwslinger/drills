{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "random.seed(11)\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this notebook, I'll be be using a database of credit card transactions to build an accurate predictor of whether a transaction is fraudulent.  The dataset, downloaded from Kaggle, is described in detail below.\n",
    "\n",
    "Thinkful's instructions: \"Using this credit card fraud dataset develop an algorithm to predict fraud. Prioritize correctly finding fraud rather than correctly labeling non-fraudulent transactions.\"  In light of this, my best metric will be the F1-score, since it doesn't take the number of true negatives into account (whereas an evenly balanced metric like theMatthews Coefficient does).\n",
    "\n",
    "\n",
    "# About the data\n",
    "\n",
    "From Kaggle:\n",
    "\n",
    "\"The dataset contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "\"It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\"\n",
    "\n",
    "It's particularly noteworthy that the dataset does not include an identity feature (so that pattern analysis could be personalized to each user).  The source does not specify whether this was included in the raw data (before PCA) or whether a version of PCA was used that could include a categorical variable of that nature.\n",
    "\n",
    "# Methods\n",
    "\n",
    "To begin with, I'll try a few general classifiers out of the box: logistic regression, random forest, and gradient boost.  Then I'll try a couple specific anomaly detection algorithms.  Of these models, I'll take the model that seems to be performing best and tweak it for optimization.\n",
    "\n",
    "Because the data is already a aset of Principal Components (except for Amount and Time), there's no real opportunity for feature engineering, so my experimentations here will all be algorithmic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and examine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_df = pd.read_csv(\"practice/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 31)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(credit_df.shape)\n",
    "credit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean transaction amount (Euros):  88.35\n",
      "Median transaction amount (Euros):  22.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean transaction amount (Euros): \", round(credit_df.Amount.mean(), 2))\n",
    "print(\"Median transaction amount (Euros): \", credit_df.Amount.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = credit_df.drop('Class', axis=1)\n",
    "y = credit_df['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial model trials\n",
    "\n",
    "## Model 1: logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.63\n",
      "Test score:  0.67\n",
      "False negatives:  33\n",
      "False positives:  45\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "lreg = LogisticRegression(penalty='l2')\n",
    "lreg.fit(X_train, y_train)\n",
    "train_pred = lreg.predict(X_train)\n",
    "test_pred = lreg.predict(X_test)\n",
    "\n",
    "false_neg = sum((y_test - test_pred) == -1)\n",
    "false_pos = sum((y_test - test_pred) == 1)\n",
    "print(\"Train score: \", round(f1_score(y_train, train_pred), 3))\n",
    "print(\"Test score: \", round(f1_score(y_test, test_pred),3))\n",
    "print(\"False negatives: \", false_neg)\n",
    "print('False positives: ', false_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.98\n",
      "Test score:  0.87\n",
      "False negatives:  4\n",
      "False positives:  25\n",
      "Elapsed time (s):  46.73529767990112\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from time import time\n",
    "start = time()\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=20)\n",
    "rfc.fit(X_train, y_train)\n",
    "train_pred = rfc.predict(X_train)\n",
    "test_pred = rfc.predict(X_test)\n",
    "\n",
    "false_neg = sum((y_test - test_pred) == -1)\n",
    "false_pos = sum((y_test - test_pred) == 1)\n",
    "print(\"Train score: \", round(f1_score(y_train, train_pred), 3))\n",
    "print(\"Test score: \", round(f1_score(y_test, test_pred),3))\n",
    "print(\"False negatives: \", false_neg)\n",
    "print('False positives: ', false_pos)\n",
    "print('Elapsed time (s): ', time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Gradient Boosted Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.827\n",
      "Test score:  0.82\n",
      "False negatives:  36\n",
      "False positives:  8\n",
      "Elapsed time (s):  132.13475608825684\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "start = time()\n",
    "\n",
    "gbc = GradientBoostingClassifier()\n",
    "gbc.fit(X_train, y_train)\n",
    "train_pred = gbc.predict(X_train)\n",
    "test_pred = gbc.predict(X_test)\n",
    "\n",
    "false_pos = sum((y_test - test_pred) == -1)\n",
    "false_neg = sum((y_test - test_pred) == 1)\n",
    "print(\"Train score: \", round(f1_score(y_train, train_pred), 3))\n",
    "print(\"Test score: \", round(f1_score(y_test, test_pred),3))\n",
    "print(\"False negatives: \", false_neg)\n",
    "print('False positives: ', false_pos)\n",
    "print('Elapsed time (s): ', time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Isolated Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.292\n",
      "Test score:  0.256\n",
      "False negatives:  94\n",
      "False positives:  109\n",
      "Elapsed time (s):  55.993184089660645\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "start = time()\n",
    "fraud_ratio = y_train.sum()/len(y_train)\n",
    "iso = IsolationForest(n_estimators=200, contamination=fraud_ratio, bootstrap=True)\n",
    "iso.fit(X_train, y_train)\n",
    "train_pred = iso.predict(X_train)\n",
    "test_pred = iso.predict(X_test)\n",
    "\n",
    "# convert +1/-1 output to 1/0:\n",
    "train_pred = (1-train_pred)/2\n",
    "test_pred = (1-test_pred)/2\n",
    "\n",
    "false_pos = sum((y_test - test_pred) == -1)\n",
    "false_neg = sum((y_test - test_pred) == 1)\n",
    "print(\"Train score: \", round(f1_score(y_train, train_pred), 3))\n",
    "print(\"Test score: \", round(f1_score(y_test, test_pred),3))\n",
    "print(\"False negatives: \", false_neg)\n",
    "print('False positives: ', false_pos)\n",
    "print('Elapsed time (s): ', time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5: Local Outlier Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.05\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "start = time()\n",
    "lof = LocalOutlierFactor(contamination=fraud_ratio)\n",
    "pred = lof.fit_predict(X)\n",
    "\n",
    "# convert +1/-1 output to 1/0:\n",
    "pred = (1-pred)/2\n",
    "\n",
    "print(\"Score: \", round(f1_score(y, pred), 3))\n",
    "print('Elapsed time (s): ', time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model choice conclusions\n",
    "\n",
    "Bafflingly, while the Random Forest and Gradient Boosted Trees performed pretty well straight out of the box, both of the specific anomaly detection models scored miserably low (f1-scores of 0.26 and 0.05).  Why are these performing so badly?\n",
    "\n",
    "Probably the next logical outlier detectio nmodel to try would be a One-Class SVM. However, given the dismal results of anomaly detection algorithms so far, for the purposes of this exercise I'm going to run with Random Forest and Gradient Boosted Tree and try some optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Random Forest\n",
    "\n",
    "First, let's run a hyperparameter sweep and see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report utility function from http://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed seconds:  7492.5006330013275\n",
      "Model with rank: 1\n",
      "Mean validation score: 1.000 (std: 0.000)\n",
      "Parameters: {'random_state': 11, 'n_estimators': 40, 'min_samples_split': 10, 'max_features': 9, 'max_depth': 20, 'criterion': 'entropy', 'bootstrap': False}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 1.000 (std: 0.000)\n",
      "Parameters: {'random_state': 11, 'n_estimators': 40, 'min_samples_split': 2, 'max_features': 9, 'max_depth': 20, 'criterion': 'entropy', 'bootstrap': True}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 1.000 (std: 0.000)\n",
      "Parameters: {'random_state': 11, 'n_estimators': 40, 'min_samples_split': 2, 'max_features': 12, 'max_depth': 20, 'criterion': 'gini', 'bootstrap': False}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "rfc_2 = RandomForestClassifier()\n",
    "\n",
    "param_dist = {\"n_estimators\":[40],\n",
    "              \"random_state\":[11],\n",
    "              \"max_depth\": [4,8,20],\n",
    "              \"max_features\": [6,9,12,20],\n",
    "              \"min_samples_split\": [2,10,20],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(rfc_2, param_distributions=param_dist, n_iter=25)\n",
    "start = time()\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Elapsed seconds: \", time()-start)\n",
    "report(random_search.cv_results_, n_top=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have several equally good scores that are all somewhat overfit.  Let's take these general parameters and look at our test score, false negatives, and false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  1.0\n",
      "Test score:  0.892\n",
      "False negatives:  22\n",
      "False positives:  4\n",
      "Elapsed time (s):  148.17836117744446\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "params = {'random_state': 11, \n",
    "          'n_estimators': 40, \n",
    "          'min_samples_split': 10, \n",
    "          'max_features': 9, \n",
    "          'max_depth': 20, \n",
    "          'criterion': 'entropy', \n",
    "          'bootstrap': False}\n",
    "\n",
    "rfc_opt = RandomForestClassifier(**params)\n",
    "rfc_opt.fit(X_train, y_train)\n",
    "train_pred = rfc_opt.predict(X_train)\n",
    "test_pred = rfc_opt.predict(X_test)\n",
    "\n",
    "false_pos = sum((y_test - test_pred) == -1)\n",
    "false_neg = sum((y_test - test_pred) == 1)\n",
    "print(\"Train score: \", round(f1_score(y_train, train_pred), 3))\n",
    "print(\"Test score: \", round(f1_score(y_test, test_pred),3))\n",
    "print(\"False negatives: \", false_neg)\n",
    "print('False positives: ', false_pos)\n",
    "print('Elapsed time (s): ', time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The good news here is that our f1-score is pretty solid at 89%.  The bad news is that we still have 22 false negatives (out of 129 actual negatives), which means we're misidentifying 17% of fraudulent transactions.  Given the cost of failing to identify fraudulent transactions, this 83% fraud detection probably isn't good enough.\n",
    "\n",
    "We may be able to improve on this by changing our algorithm.  By adjusting the class weights of the random tree, we can heavily prioritize correctly identifying positive cases (the minority case) over negative cases.  Let's try over-weighting fraud identification by increasing powers of 10 and see what happens to our false negatives and our f1-scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time (s):  429.8313000202179\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "scores = []\n",
    "false_neg_counts = []\n",
    "false_pos_counts = []\n",
    "balance_exps = [0, 2, 4, 6, 8, 10, 12]\n",
    "for balance_exp in balance_exps:\n",
    "    params = {'random_state': 11, \n",
    "              'n_estimators': 20, \n",
    "              'min_samples_split': 10, \n",
    "              'max_features': 9, \n",
    "              'max_depth': 20, \n",
    "              'criterion': 'entropy', \n",
    "              'bootstrap': False,\n",
    "              'class_weight': {0:1, 1:10**balance_exp}\n",
    "             }\n",
    "\n",
    "    rfc = RandomForestClassifier(**params)\n",
    "    rfc.fit(X_train, y_train)\n",
    "    train_pred = rfc.predict(X_train)\n",
    "    test_pred = rfc.predict(X_test)\n",
    "\n",
    "    false_neg_counts.append( sum((y_test - test_pred) == 1) )\n",
    "    false_pos_counts.append( sum((y_test - test_pred) == -1) )\n",
    "    scores.append( round(f1_score(y_train, train_pred), 3) )\n",
    "\n",
    "print('Elapsed time (s): ', time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1154dd9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(balance_exps, false_neg_counts, label='false negatives')\n",
    "plt.plot(balance_exps, false_pos_counts, label='false positives')\n",
    "plt.plot(balance_exps, [s*10 for s in scores], label='f1-scores * 10')\n",
    "plt.title(\"class_weight exponent vs model metrics\")\n",
    "plt.xlabel('log of weight imbalance')\n",
    "plt.ylabel('performance metrics')\n",
    "plt.legend()\n",
    "plt.ylim(0,40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the tradeoff in accuracy is way too steep here: our f1-score drops below 0.5 before we see any significant reduction in false negatives.\n",
    "\n",
    "For now, then, our best try is 83% fraud detection with an f1-score of 89%.  Let's table that for now and try optimizing our gradient boosted tree model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Gradient Boosted Tree\n",
    "\n",
    "We'll start by optimizing all parameters except the computation-intensive n_estimators and learning_rate, then hold the others constant while we tweak those two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed seconds:  859.3788621425629\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.999 (std: 0.000)\n",
      "Parameters: {'subsample': 1, 'random_state': 11, 'n_estimators': 50, 'max_features': 8, 'max_depth': 6, 'loss': 'exponential', 'learning_rate': 0.1}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.999 (std: 0.000)\n",
      "Parameters: {'subsample': 1, 'random_state': 11, 'n_estimators': 50, 'max_features': 6, 'max_depth': 6, 'loss': 'exponential', 'learning_rate': 0.1}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.999 (std: 0.000)\n",
      "Parameters: {'subsample': 0.6, 'random_state': 11, 'n_estimators': 50, 'max_features': 8, 'max_depth': 8, 'loss': 'exponential', 'learning_rate': 0.1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from time import time\n",
    "\n",
    "gbc_2 = GradientBoostingClassifier()\n",
    "\n",
    "param_dist = {\n",
    "    'loss': ['deviance','exponential'],\n",
    "    'n_estimators': [50], # we'll change this next time\n",
    "    'learning_rate': [0.1], # ditto\n",
    "    'max_depth': [2,4,6,8],\n",
    "    'max_features': [2,4,6,8],\n",
    "    'subsample': [0.1, 0.3, 0.6, 1],\n",
    "    'random_state': [11]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(gbc_2, param_distributions=param_dist, n_iter=25)\n",
    "start = time()\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Elapsed seconds: \", time()-start)\n",
    "report(random_search.cv_results_, n_top=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed seconds:  3412.5153291225433\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.999 (std: 0.000)\n",
      "Parameters: {'subsample': 1, 'random_state': 11, 'n_estimators': 100, 'max_features': 6, 'max_depth': 6, 'loss': 'exponential', 'learning_rate': 0.2}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.999 (std: 0.000)\n",
      "Parameters: {'subsample': 1, 'random_state': 11, 'n_estimators': 200, 'max_features': 6, 'max_depth': 6, 'loss': 'exponential', 'learning_rate': 0.2}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.999 (std: 0.000)\n",
      "Parameters: {'subsample': 1, 'random_state': 11, 'n_estimators': 100, 'max_features': 6, 'max_depth': 6, 'loss': 'exponential', 'learning_rate': 0.1}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.999 (std: 0.000)\n",
      "Parameters: {'subsample': 1, 'random_state': 11, 'n_estimators': 200, 'max_features': 6, 'max_depth': 6, 'loss': 'exponential', 'learning_rate': 0.05}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from time import time\n",
    "param_dist = {\n",
    "    'loss': ['exponential'],\n",
    "    'n_estimators': [50,100,200],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [6],\n",
    "    'max_features': [6],\n",
    "    'subsample': [1],\n",
    "    'random_state': [11]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(gbc_2, param_distributions=param_dist, n_iter=10)\n",
    "start = time()\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Elapsed seconds: \", time()-start)\n",
    "report(random_search.cv_results_, n_top=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.983\n",
      "Test score:  0.876\n",
      "False negatives:  27\n",
      "False positives:  4\n",
      "Elapsed time (s):  90.07184290885925\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "params = {'random_state': 11, 'n_estimators': 100, 'max_features': 6, 'max_depth': 6, 'loss': 'exponential', 'learning_rate': 0.2}\n",
    "\n",
    "gbc_3 = GradientBoostingClassifier(**params)\n",
    "gbc_3.fit(X_train, y_train)\n",
    "train_pred = gbc_3.predict(X_train)\n",
    "test_pred = gbc_3.predict(X_test)\n",
    "\n",
    "false_pos = sum((y_test - test_pred) == -1)\n",
    "false_neg = sum((y_test - test_pred) == 1)\n",
    "print(\"Train score: \", round(f1_score(y_train, train_pred), 3))\n",
    "print(\"Test score: \", round(f1_score(y_test, test_pred),3))\n",
    "print(\"False negatives: \", false_neg)\n",
    "print('False positives: ', false_pos)\n",
    "print('Elapsed time (s): ', time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that gradient boosted tree will yield basically the same results as random forest.  Since these results have one fewer false negative, we'll use gradient boosted tree as our top performer moving forward.  But they're basically identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final boosted tree test\n",
    "\n",
    "Our best model on our test set has been our gradient boosted tree.  Let's make sure this replicates across different random splits of our data (i.e., make sure it's not overfit to our test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False negatives mean:  128.5\n",
      "False negatives variance:  30.25\n",
      "False positives mean:  112.75\n",
      "Mean f1-score: 0.002\n",
      "Elapsed time (s):  85.12541890144348\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "false_neg_counts = []\n",
    "false_pos_counts = []\n",
    "scores = []\n",
    "\n",
    "for i in range(4):\n",
    "    random.seed(a=None)\n",
    "    X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "    start = time()\n",
    "    params = {'random_state': 11, 'n_estimators': 100, 'max_features': 6, 'max_depth': 6, 'loss': 'exponential', 'learning_rate': 0.2}\n",
    "\n",
    "    gbc_4 = GradientBoostingClassifier(**params)\n",
    "    gbc_4.fit(X_train, y_train)\n",
    "    train_pred = gbc_4.predict(X_train)\n",
    "    test_pred = gbc_4.predict(X_test)\n",
    "    \n",
    "    false_pos = sum((y_test2 - test_pred) == -1)\n",
    "    false_negs = sum((y_test2 - test_pred) == 1)\n",
    "    score = round(f1_score(y_test2, test_pred),3)\n",
    "    \n",
    "    false_pos_counts.append(false_pos)\n",
    "    false_neg_counts.append(false_negs)\n",
    "    scores.append(score)\n",
    "    \n",
    "print(\"False negatives mean: \", round(np.mean(false_neg_counts),3) )\n",
    "print(\"False negatives variance: \", round(np.var(false_neg_counts),3) )\n",
    "print(\"False positives mean: \", round(np.mean(false_pos_counts),3) )\n",
    "print(\"Mean f1-score:\", round(np.mean(scores), 3) )\n",
    "print('Elapsed time (s): ', time()-start)\n",
    "random.seed(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[113, 130, 123, 123]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_neg_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why on earth are these results coming back like this?  Why is the time so short, and why are the metrics so bad?  I've read through this line by line twice and can't make heads or tails of it.  For the moment, I'll stick with the results of the previous single test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In light of these results, my recommendation would probably be for the bank in this case to adopt a hybrid model.\n",
    "\n",
    "The primary model would be the gradient boosted tree above, which catches 83% of fraudulent transactions with an almost nonexistent rate of false positives.  Transactions flagged by this fraud detector should be denied.\n",
    "\n",
    "The secondary should be a model that flags nearly all of the fraudulent transactions at the cost of also incorrectly flagging many non-fraudulent transactions.  We might, for instance, use our heavily weighted random forest model, which flagged 94% of fraudulent transactions, at the cost of having 98% of its flags were false flags.  For transactions flagged only by the seondary model, the transaction should only be denied if it is above a certain amount (e.g., 2,000 USD) above which the risk of a fraudulent transaction is unacceptable, or if it is the third or more consecutive secondary-flagged transaction (which would only occur in approximately 0.1 percent of all transactions).\n",
    "\n",
    "Together, these models will catch 83% of fraudulent transactions, and halt an additional 11% of fraudulent transactions if they are large transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "<ol>\n",
    "    <li>Why do the specific outlier factors (Local Outlier Factor and One-class SVM) perform so badly here?</li>\n",
    "     <li>What's going on with the haywire results of the final boosted tree test above?</li>\n",
    "    <li>It strikes me that this is a case where I probably should have divided my dataset three ways into a training set, cv set, and test set, rather than a simple train_test split.  I compensated for this by testing against several new random splits at the end, which is almost (but not totally) as good.  Should I have done a three-way split here?  Should I usually do a three-way split?</li>\n",
    "    <li>Why do the weight adjustments below not behave as expected?  Are there better ways to prioritize eliminating false positives?  (I get the impression that thresholding is not ideal.  And according to this paper, weighting and subsampling perform almost identically: http://statistics.berkeley.edu/sites/default/files/tech-reports/666.pdf)</li>\n",
    "</ol>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Everything below here is scratch -- ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  1.0\n",
      "Test score:  0.87\n"
     ]
    }
   ],
   "source": [
    "# stack 1: final boost like boosted tree\n",
    "rfc = RandomForestClassifier(n_estimators=40)\n",
    "rfc.fit(X_train, y_train)\n",
    "train_pred = rfc.predict(X_train)\n",
    "test_pred = rfc.predict(X_test)\n",
    "errors = train_pred-y_train\n",
    "\n",
    "rfc2 = RandomForestClassifier(n_estimators=40)\n",
    "rfc2.fit(X_train, errors)\n",
    "error_train_pred = rfc2.predict(X_train)\n",
    "error_test_pred = rfc2.predict(X_test)\n",
    "\n",
    "final_train_pred = train_pred - error_train_pred\n",
    "final_test_pred = test_pred - error_test_pred\n",
    "print(\"Train score: \", round(f1_score(y_train, final_train_pred), 3))\n",
    "print(\"Test score: \", round(f1_score(y_test, final_test_pred),3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRY STACKING HERE (one boost after the RF - or more?)\n",
    "\n",
    "also JUST REALIZED that NNs are just stacked linear regressions.  What happens if you stack RFs in multiple layers???\n",
    "\n",
    "OR BETTER YET: figure out how to combine RFs into a NN-analogous structure\n",
    "A backprop equivalent is, I think, the key here.  Could you use something like gradient boost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack 2: use outputs from trees as input\n",
    "# part 1: generate tree predictions\n",
    "train_predictions = []\n",
    "test_predictions = []\n",
    "forests = 20\n",
    "for i in range(forests):\n",
    "    rfc = RandomForestClassifier(n_estimators=5)\n",
    "    rfc.fit(X_train, y_train)\n",
    "    train_predictions.append(rfc.predict(X_train))\n",
    "    test_predictions.append(rfc.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimental part 3: double-stack, like a NN\n",
    "train_predictions2 = []\n",
    "test_predictions2 = []\n",
    "forests = 20\n",
    "for i in range(forests):\n",
    "    rfc = RandomForestClassifier(n_estimators=5)\n",
    "    rfc.fit(train_predictions, y_train)\n",
    "    train_predictions2.append(rfc.predict(train_predictions))\n",
    "    test_predictions2.append(rfc.predict(test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 2: train model on those predictions\n",
    "rfc = RandomForestClassifier(n_estimators=80)\n",
    "rfc.fit(predictions, y_train)\n",
    "train_pred = rfc.predict(train_predictions)\n",
    "test_pred = rfc.predict(test_predictions)\n",
    "\n",
    "false_pos = sum((y_test - test_pred) == -1)\n",
    "false_neg = sum((y_test - test_pred) == 1)\n",
    "print(\"Train score: \", round(f1_score(y_train, train_pred), 3))\n",
    "print(\"Test score: \", round(f1_score(y_test, test_pred),3))\n",
    "print(\"False negatives: \", false_neg)\n",
    "print('False positives: ', false_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fun, take just the top two PCs and plot the gradient and shape as in the sklearn examples.  This might also help diagnose the problems with my anomaly detection algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
