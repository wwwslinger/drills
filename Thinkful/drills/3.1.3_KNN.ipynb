{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From the curriculum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "music = pd.DataFrame()\n",
    "music['duration'] = [184, 134, 243, 186, 122, 197, 294, 382, 102, 264, \n",
    "                     205, 110, 307, 110, 397, 153, 190, 192, 210, 403,\n",
    "                     164, 198, 204, 253, 234, 190, 182, 401, 376, 102]\n",
    "music['loudness'] = [18, 34, 43, 36, 22, 9, 29, 22, 10, 24, \n",
    "                     20, 10, 17, 51, 7, 13, 19, 12, 21, 22,\n",
    "                     16, 18, 4, 23, 34, 19, 14, 11, 37, 42]\n",
    "music['bpm'] = [ 105, 90, 78, 75, 120, 110, 80, 100, 105, 60,\n",
    "                  70, 105, 95, 70, 90, 105, 70, 75, 102, 100,\n",
    "                  100, 95, 90, 80, 90, 80, 100, 105, 70, 65]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "First let me state that these two models are fantastically awful. There doesn't seem to be much of a relationship. It's all very poor. However the increased variance in the weighted model is interesting.\n",
    "\n",
    "Why don't you add the other feature and mess around with  kk  and weighting to see if you can do any better than we've done so far?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unweighted Accuracy: -0.69 (+/- 1.03)\n",
      "Weighted Accuracy: -0.59 (+/- 1.75)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "## Your model here.\n",
    "# Run the same model, this time with weights.\n",
    "knn = neighbors.KNeighborsRegressor(n_neighbors=3)\n",
    "knn_w = neighbors.KNeighborsRegressor(n_neighbors=3, weights='distance')\n",
    "X = pd.DataFrame(music[[\"duration\", \"loudness\"]])\n",
    "Y = music.bpm\n",
    "knn.fit(X, Y)\n",
    "knn_w.fit(X, Y)\n",
    "\n",
    "# Set up our prediction line.\n",
    "T1 = np.arange(0, 50, 0.1)[:, np.newaxis]\n",
    "T2 = np.arange(0, 50, 0.1)[:, np.newaxis]\n",
    "T = np.concatenate((T1, T2), axis=1)\n",
    "\n",
    "Y_ = knn_w.predict(T)\n",
    "\n",
    "# plt.scatter(X, Y, c='k', label='data')\n",
    "# plt.plot(T, Y_, c='g', label='prediction')\n",
    "# plt.legend()\n",
    "# plt.title('K=10, Weighted')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "score = cross_val_score(knn, X, Y, cv=5)\n",
    "print(\"Unweighted Accuracy: %0.2f (+/- %0.2f)\" % (score.mean(), score.std() * 2))\n",
    "score_w = cross_val_score(knn_w, X, Y, cv=5)\n",
    "print(\"Weighted Accuracy: %0.2f (+/- %0.2f)\" % (score_w.mean(), score_w.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR K = 1\n",
      "Unweighted Accuracy: -0.35 (+/- 0.62)\n",
      "Weighted Accuracy: -0.35 (+/- 0.62)\n",
      "RESULTS FOR K = 2\n",
      "Unweighted Accuracy: -0.46 (+/- 0.50)\n",
      "Weighted Accuracy: -0.19 (+/- 0.50)\n",
      "RESULTS FOR K = 3\n",
      "Unweighted Accuracy: -0.52 (+/- 0.45)\n",
      "Weighted Accuracy: -0.19 (+/- 0.52)\n",
      "RESULTS FOR K = 4\n",
      "Unweighted Accuracy: -0.42 (+/- 0.17)\n",
      "Weighted Accuracy: -0.11 (+/- 0.27)\n",
      "RESULTS FOR K = 5\n",
      "Unweighted Accuracy: -0.41 (+/- 0.49)\n",
      "Weighted Accuracy: -0.09 (+/- 0.24)\n",
      "RESULTS FOR K = 6\n",
      "Unweighted Accuracy: -0.31 (+/- 0.38)\n",
      "Weighted Accuracy: -0.06 (+/- 0.21)\n",
      "RESULTS FOR K = 7\n",
      "Unweighted Accuracy: -0.26 (+/- 0.19)\n",
      "Weighted Accuracy: -0.03 (+/- 0.19)\n",
      "RESULTS FOR K = 8\n",
      "Unweighted Accuracy: -0.26 (+/- 0.23)\n",
      "Weighted Accuracy: -0.03 (+/- 0.24)\n",
      "RESULTS FOR K = 9\n",
      "Unweighted Accuracy: -0.18 (+/- 0.21)\n",
      "Weighted Accuracy: 0.00 (+/- 0.17)\n",
      "RESULTS FOR K = 10\n",
      "Unweighted Accuracy: -0.08 (+/- 0.18)\n",
      "Weighted Accuracy: 0.02 (+/- 0.17)\n",
      "RESULTS FOR K = 11\n",
      "Unweighted Accuracy: -0.10 (+/- 0.07)\n",
      "Weighted Accuracy: 0.02 (+/- 0.19)\n",
      "RESULTS FOR K = 12\n",
      "Unweighted Accuracy: -0.06 (+/- 0.11)\n",
      "Weighted Accuracy: 0.03 (+/- 0.15)\n",
      "RESULTS FOR K = 13\n",
      "Unweighted Accuracy: -0.10 (+/- 0.15)\n",
      "Weighted Accuracy: 0.03 (+/- 0.14)\n",
      "RESULTS FOR K = 14\n",
      "Unweighted Accuracy: -0.07 (+/- 0.10)\n",
      "Weighted Accuracy: 0.03 (+/- 0.14)\n",
      "RESULTS FOR K = 15\n",
      "Unweighted Accuracy: -0.08 (+/- 0.09)\n",
      "Weighted Accuracy: 0.03 (+/- 0.16)\n",
      "RESULTS FOR K = 16\n",
      "Unweighted Accuracy: -0.06 (+/- 0.08)\n",
      "Weighted Accuracy: 0.04 (+/- 0.17)\n",
      "RESULTS FOR K = 17\n",
      "Unweighted Accuracy: -0.08 (+/- 0.14)\n",
      "Weighted Accuracy: 0.03 (+/- 0.18)\n",
      "RESULTS FOR K = 18\n",
      "Unweighted Accuracy: -0.04 (+/- 0.19)\n",
      "Weighted Accuracy: 0.04 (+/- 0.19)\n",
      "RESULTS FOR K = 19\n",
      "Unweighted Accuracy: -0.04 (+/- 0.11)\n",
      "Weighted Accuracy: 0.04 (+/- 0.19)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# set up data\n",
    "X = pd.DataFrame(music[[\"duration\", \"loudness\"]])\n",
    "Y = music.bpm\n",
    "T1 = np.arange(0, 50, 0.1)[:, np.newaxis]\n",
    "T2 = np.arange(0, 50, 0.1)[:, np.newaxis]\n",
    "T = np.concatenate((T1, T2), axis=1)\n",
    "\n",
    "def testruns(n):\n",
    "    for i in range(1,n):\n",
    "        knn = neighbors.KNeighborsRegressor(n_neighbors=i)\n",
    "        knn_w = neighbors.KNeighborsRegressor(n_neighbors=i, weights='distance')\n",
    "\n",
    "        print(\"RESULTS FOR K = \" + str(i))\n",
    "        score = cross_val_score(knn, X, Y, cv=3)\n",
    "        print(\"Unweighted Accuracy: %0.2f (+/- %0.2f)\" % (score.mean(), score.std() * 2))\n",
    "        score_w = cross_val_score(knn_w, X, Y, cv=3)\n",
    "        print(\"Weighted Accuracy: %0.2f (+/- %0.2f)\" % (score_w.mean(), score_w.std() * 2))\n",
    "        \n",
    "testruns(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results:\n",
    "\n",
    "Adding the second feature (duration) actually hurt the already terrible R-squared scores.  The peak score using both features was 0.04, barely distinguishable from a flat line across the mean.\n",
    "\n",
    "An interesting result is that both accuracy scores improve with a higher K (until the sample size becomes too small to allow any higher).  My theory is that using more datapoints for each estimate, especially in a small dataset like this, helps combat overfitting.  This trend would persist for larger datasets, but more weakly, and topping out at a lower K-value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
